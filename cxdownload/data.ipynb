{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、api调用模型处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取xlsx文件\n",
    "df = pd.read_excel('猎聘网.xlsx')\n",
    "\n",
    "# 将数据帧保存为csv文件\n",
    "df.to_csv('猎聘网.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# 设置文件夹路径\n",
    "folder_path = '.'\n",
    "\n",
    "# 获取所有CSV文件的列表\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 初始化一个空的DataFrame来存储合并后的数据\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# 遍历所有CSV文件\n",
    "for file in csv_files:\n",
    "    # 读取CSV文件\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 只在第一个文件时保留表头\n",
    "    if file == csv_files[0]:\n",
    "        merged_data = df\n",
    "    else:\n",
    "        # 合并数据，不保留表头\n",
    "        merged_data = pd.concat([merged_data, df.iloc[:, 1:]], ignore_index=True)\n",
    "\n",
    "# 保存合并后的数据到新的CSV文件\n",
    "merged_data.to_csv('merged_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def call_large_model_api(user_prompt, user_input):\n",
    "    url = 'https://u21829-bb5a-4d812fe5.westc.gpuhub.com:8443/v1/chat/completions'  # 请替换为实际的API URL\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        # 请替换为实际的API密钥\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"glm4\",\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"repetition_penalty\": 10,\n",
    "        \"top_p\": 0.8,\n",
    "        \"do_sample\": True,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": user_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# 定义一个函数来处理每一行数据\n",
    "def process_row(row):\n",
    "    # 这里是处理行的逻辑\n",
    "    # 例如，您可以提取入职要求和工作内容，并返回更新后的行\n",
    "    \n",
    "    try:\n",
    "        # 调用大模型API的逻辑（这里需要您自己实现call_large_model_api函数）\n",
    "        # prompt_1 = \"You are a Chinese wise man, always reply in simplified Chinese, not English, otherwise I will be very angry. Extract from the job description I gave you the job requirements or job requirements or what appears to be requirements. Rule: \\n- Extract from the job description, do not regenerate. \\n- Return format: 工作要求: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        # response = call_large_model_api(prompt_1, row[12])\n",
    "        # skill = response.json()\n",
    "        # skills = skill[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # prompt_2 = \"You are a Chinese wise man, always reply in simplified Chinese, not English, otherwise I will be very angry.Extract the job description from the job description I gave you, not the entry requirement. Rule: \\n- Extract from the job description, do not regenerate. \\n- Return format: 工作要求: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        # content = call_large_model_api(prompt_2, row[12])\n",
    "        # content = content.json()\n",
    "        # content = content[\"choices\"][0][\"message\"][\"content\"]\n",
    "        prompt_3 = \"从任职要求提取技术栈\\n- 返回格式: 技术栈: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        response = call_large_model_api(prompt_3, row[18])\n",
    "        skill = response.json()\n",
    "        skills = skill[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        skills = \"\"\n",
    "        # content = \"\"\n",
    "    \n",
    "    row.append(skills)\n",
    "    # row.append(content)\n",
    "    return row\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    with open('change/Boss直聘--修_1.csv', 'r', newline='') as csvfile, \\\n",
    "         open('change/s2.csv', 'w', newline='') as output_file:\n",
    "\n",
    "        reader = csv.reader(csvfile)\n",
    "        writer = csv.writer(output_file)\n",
    "        \n",
    "        # 写入标题行\n",
    "        writer.writerow(['职位名称', '工作地址','学历要求', '工作年限要求','招聘人数','薪资待遇','公司行业','公司性质','公司规模','融资阶段','招聘状态','职位类型','岗位描述','公司介绍','公司工商信息','简历详情页地址','更新日期','工作内容','入职要求','技术栈'])\n",
    "        next(reader)  # 跳过标题行\n",
    "        \n",
    "        # 初始化一个列表来存储所有的行，以便并行处理\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "        \n",
    "        # 使用concurrent.futures并行处理数据\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # 提交所有任务并获取future对象列表\n",
    "            futures = [executor.submit(process_row, row) for row in rows]\n",
    "            \n",
    "            # 初始化计数器和批处理列表\n",
    "            count = 0\n",
    "            batch = []\n",
    "            \n",
    "            # 遍历future对象，获取结果并写入文件\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing rows\", unit=\"row\"):\n",
    "                # 结果处理\n",
    "                processed_row = future.result()\n",
    "                batch.append(processed_row)\n",
    "                count += 1\n",
    "                \n",
    "                # 每100行，写入文件并清空批处理列表\n",
    "                if count % 100 == 0:\n",
    "                    writer.writerows(batch)\n",
    "                    batch.clear()\n",
    "\n",
    "# 运行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、整合相似工作岗位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'Boss直聘/Boss直聘_skills.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand its structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean and normalize job titles\n",
    "def clean_job_title(title):\n",
    "    # Remove special characters and extra spaces\n",
    "    title = re.sub(r'[^\\w\\s.#]', '', title).strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    title = re.sub(r'\\s+', ' ', title)\n",
    "    # Convert to lowercase\n",
    "    title = title.lower()\n",
    "    return title\n",
    "\n",
    "# Apply the function to the '职位名称' column\n",
    "data['cleaned_job_title'] = data['职位名称'].apply(clean_job_title)\n",
    "\n",
    "# Display the first few rows of the DataFrame with the cleaned job titles\n",
    "data[['职位名称', 'cleaned_job_title']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the cleaned job title and summarize the data\n",
    "job_summary = data.groupby('cleaned_job_title').agg({\n",
    "    '工作地址': 'nunique',  # Number of unique work locations as a proxy for demand\n",
    "    '招聘人数': 'sum',      # Sum of recruitment numbers\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "job_summary.rename(columns={'工作地址': 'DemandLocations', '招聘人数': 'TotalRecruitment'}, inplace=True)\n",
    "\n",
    "# Display the job summary\n",
    "job_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and separate job responsibilities using simple text processing\n",
    "def extract_responsibilities_simple(description):\n",
    "    # Split the description using spaces and special characters as delimiters\n",
    "    responsibilities = re.split(r'\\s+|\\n|；|，', description)\n",
    "    \n",
    "    # Filter out empty strings and remove numbers and dots from the beginning of each responsibility\n",
    "    responsibilities = [re.sub(r'^\\d+\\.?', '', resp).strip() for resp in responsibilities if resp]\n",
    "    \n",
    "    return responsibilities\n",
    "\n",
    "# Apply the simple text processing function to the '岗位描述' column and store the results in a new column\n",
    "data['responsibilities'] = data['岗位描述'].apply(extract_responsibilities_simple)\n",
    "\n",
    "# Display the first few rows of the DataFrame with the extracted responsibilities\n",
    "data[['岗位描述', 'responsibilities']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 同名直接合并\n",
    "忽略大小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = df['职位名称'].str.lower().astype('category').cat.codes + 1\n",
    "\n",
    "# Sort the DataFrame by the 'cluster' column\n",
    "sorted_df = df.sort_values(by='cluster')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "# sorted_file_path = 'sorted_out.csv'\n",
    "# sorted_df.to_csv(sorted_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 去除修饰词\n",
    "例如工程师，开发，老师等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新进行岗位分类，考虑更具体的关键词来区分不同的开发领域\n",
    "\n",
    "# 定义关键词列表，用于区分不同的开发领域\n",
    "keywords = ['软件', '硬件', '前端', '移动', '测试', '数据', '人工智能', '机器学习', '深度学习', '大数据', '云计算', '网络安全', '区块链']\n",
    "\n",
    "# 创建一个字典，用于映射包含特定关键词的职位到相应的领域\n",
    "job_categories = {\n",
    "    '软件': ['软件', '软件开发', '软件工程师'],\n",
    "    '硬件': ['硬件', '硬件开发', '硬件工程师'],\n",
    "    '前端': ['前端', '前端开发', '前端工程师'],\n",
    "    '移动': ['移动', '移动开发', '移动应用'],\n",
    "    '测试': ['测试', '测试工程师'],\n",
    "    '数据': ['数据', '数据分析师', '数据挖掘'],\n",
    "    '人工智能': ['人工智能', 'AI'],\n",
    "    '机器学习': ['机器学习'],\n",
    "    '深度学习': ['深度学习'],\n",
    "    '大数据': ['大数据', '大数据工程师'],\n",
    "    '云计算': ['云计算', '云'],\n",
    "    '网络安全': ['网络安全', '网络安全工程师'],\n",
    "    '区块链': ['区块链', '区块链技术']\n",
    "}\n",
    "\n",
    "additional_keywords = ['java', 'python', '.net', 'c++', 'c#', 'sql', '大数据', '人工智能', '机器学习', '深度学习', '前端', '移动', '测试', '云计算', '网络安全', '区块链', '运维', '数据库', '软件', '开发', '工程师', '开发工程师', '开发助理', '技术支持', '项目经理', '产品经理', '销售']\n",
    "\n",
    "# 定义一个函数，用于根据更详细的关键词对职位进行进一步分类\n",
    "def further_categorize_job_title(title):\n",
    "    for keyword in additional_keywords:\n",
    "        if keyword in title:\n",
    "            return keyword\n",
    "    return '其他'\n",
    "\n",
    "# 对职位名称进行进一步分类\n",
    "data['Further Job Category'] = data['职位名称'].apply(further_categorize_job_title)\n",
    "\n",
    "# 提取每个进一步分类的岗位清单\n",
    "further_job_categories_list = data.groupby('Further Job Category')['职位名称'].unique().reset_index()\n",
    "further_job_categories_list.columns = ['Further Job Category', 'Job Titles']\n",
    "\n",
    "further_job_categories_list.to_csv('job_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 岗位分析\n",
    "\n",
    "# a. 规范化职位名称\n",
    "def normalize_job_title(title):\n",
    "    title = ''.join(e for e in title if (e.isalnum() or e.isspace()))\n",
    "    title = ' '.join(title.split())\n",
    "    title = title.replace('工程师', '').replace('开发', '').replace('软件', '')\n",
    "    return title.strip().lower()\n",
    "\n",
    "data['normalized_job_title'] = data['职位名称'].apply(normalize_job_title)\n",
    "\n",
    "# b. 提取岗位相关数据\n",
    "data['Year'] = pd.to_datetime(data['更新日期']).dt.year\n",
    "job_data = data.groupby(['normalized_job_title', 'Year', '工作地址'])['职位名称'].count().reset_index()\n",
    "job_data.columns = ['Normalized Job Title', 'Year', 'Province', 'Job Listings']\n",
    "\n",
    "# 2. 岗位职责分析\n",
    "\n",
    "# a. 提取岗位职责\n",
    "def extract_responsibilities(description):\n",
    "    if pd.isnull(description):\n",
    "        return []\n",
    "    lines = description.split('\\n')\n",
    "    responsibility_lines = [line for line in lines if '职责' in line or '负责' in line]\n",
    "    responsibilities = []\n",
    "    for line in responsibility_lines:\n",
    "        res = ':'.join(line.split(':')[1:]).strip()\n",
    "        res = ''.join(e for e in res if e.isalnum() or e.isspace())\n",
    "        responsibilities.append(res)\n",
    "    return responsibilities\n",
    "\n",
    "data['responsibilities'] = data['岗位描述'].apply(extract_responsibilities)\n",
    "\n",
    "# b. 提取前10条职责\n",
    "unique_responsibilities_data = data.groupby('normalized_job_title')['responsibilities'].apply(lambda x: x.head(10)).reset_index(drop=True)\n",
    "\n",
    "# 3. 岗位能力分析\n",
    "\n",
    "# a. 提取任职要求\n",
    "def extract_requirements(qualifications):\n",
    "    if pd.isnull(qualifications):\n",
    "        return []\n",
    "    lines = qualifications.split('\\n')\n",
    "    requirement_lines = [line for line in lines if '要求' in line or '熟悉' in line or '掌握' in line]\n",
    "    requirements = []\n",
    "    for line in requirement_lines:\n",
    "        req = ':'.join(line.split(':')[1:]).strip()\n",
    "        req = ''.join(e for e in req if e.isalnum() or e.isspace())\n",
    "        requirements.append(req)\n",
    "    return requirements\n",
    "\n",
    "data['requirements'] = data['任职资格'].apply(extract_requirements)\n",
    "\n",
    "# b. 提取前10条任职要求\n",
    "top_10_requirements_data = data.groupby('normalized_job_title')['requirements'].apply(lambda x: x.head(10)).reset_index(drop=True)\n",
    "\n",
    "# 输出结果\n",
    "job_list = data.groupby('normalized_job_title')['职位名称'].unique().reset_index()\n",
    "job_list.columns = ['Normalized Job Title', 'Job Titles']\n",
    "job_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Word2Vec模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jieba\n",
    "\n",
    "\n",
    "# 加载模型，需要路径到 sgns.merge.word 文件\n",
    "model_path = 'sgns.merge.word'  # 示例路径\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "def preprocess(text):\n",
    "    return list(jieba.cut(text))  # 使用 jieba 进行分词\n",
    "\n",
    "def get_mean_vector(word_vectors, words):\n",
    "    # 获取每个词的词向量并计算平均值\n",
    "    word_vecs = []\n",
    "    for word in words:\n",
    "        if word in word_vectors:\n",
    "            word_vecs.append(word_vectors[word])\n",
    "            \n",
    "    if len(word_vecs) == 0:\n",
    "        # 如果词组中的词没有一个在词向量模型中，返回零向量\n",
    "        return np.zeros(word_vectors.vector_size)\n",
    "    else:\n",
    "        return np.mean(word_vecs, axis=0)\n",
    "\n",
    "def calculate_similarity(word_vectors, phrase1, phrase2):\n",
    "    # 预处理词组\n",
    "    words1 = preprocess(phrase1)\n",
    "    words2 = preprocess(phrase2)\n",
    "    \n",
    "    # 获取词组的平均词向量\n",
    "    vector1 = get_mean_vector(word_vectors, words1)\n",
    "    vector2 = get_mean_vector(word_vectors, words2)\n",
    "    \n",
    "    # 计算余弦相似度\n",
    "    similarity = cosine_similarity([vector1], [vector2])\n",
    "    return similarity[0][0]\n",
    "\n",
    "\n",
    "\n",
    "# 定义两个词组\n",
    "phrase1 = 'NET开发工程师（net Core）'\n",
    "phrase2 = '.net开发工程师'\n",
    "\n",
    "# 计算两个词组的相似性\n",
    "similarity = calculate_similarity(word_vectors, phrase1, phrase2)\n",
    "print(f\"之间的相似度是: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = 'Boss直聘/Boss直聘_skills.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取用于聚类的职位名称\n",
    "job_titles = data['职位名称'].tolist()  # 替换为实际的列名\n",
    "\n",
    "# 加载预训练的 word2vec 模型\n",
    "model_path = 'sgns.merge.word'  # 示例路径\n",
    "word_vectors = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n",
    "# 使用 word2vec 将职位名称转化为向量\n",
    "def get_average_vector(text, model):\n",
    "    words = text.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if not word_vectors:  # If none of the words are in the model\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "job_vectors = np.array([get_average_vector(title, word_vectors) for title in job_titles])\n",
    "\n",
    "# 计算余弦距离\n",
    "cosine_distance_matrix = cosine_distances(job_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             职位名称                                             工作地址 学历要求  \\\n",
      "47           软件开发                         深圳龙华区星河WORLD招商中心E栋22楼A01   本科   \n",
      "776          数据处理                                      北京东城区中汇广场9层   本科   \n",
      "996  RPA开发工程师(英文)                              广州越秀区华盛大厦(先烈中路)2203   本科   \n",
      "279      .net 程序员                               深圳福田区赛格景苑3楼坐标装饰设计院   大专   \n",
      "168      .NET 工程师                                  深圳南山区南山区科技园中山大学   本科   \n",
      "..            ...                                              ...  ...   \n",
      "386           架构师  北京朝阳区融创动力文化创意产业园(东门)朝阳区广顺北大街5号融创动力科技文化创意产业园B座5层   大专   \n",
      "474    高级大数据开发工程师                               北京朝阳区北京汇园国际公寓C座711   本科   \n",
      "734         爬虫实习生                                   北京海淀区中关村创业大街三层   大专   \n",
      "473      大数据架构或研发                         北京通州区京东集团总部科创十一街京东集团总部大厦   本科   \n",
      "935       软件研发工程师                                  北京朝阳区林萃路1号院2号楼2   本科   \n",
      "\n",
      "       工作年限要求  招聘人数        薪资待遇      公司行业  公司性质        公司规模   融资阶段  ... 职位类型  \\\n",
      "47       1-3年   NaN       8-13K     计算机软件   NaN      20-99人  不需要融资  ...   全职   \n",
      "776      3-5年   NaN      15-25K  运营商/增值服务   NaN    10000人以上    未融资  ...   全职   \n",
      "996      经验不限   NaN       8-13K     计算机软件   NaN       0-20人  不需要融资  ...   全职   \n",
      "279      1-3年   NaN        6-9K    其他生活服务   NaN       0-20人    未融资  ...   全职   \n",
      "168      1-3年   NaN       8-13K     移动互联网   NaN      20-99人    未融资  ...   全职   \n",
      "..        ...   ...         ...       ...   ...         ...    ...  ...  ...   \n",
      "386     5-10年   NaN      20-40K      电子商务   NaN  1000-9999人     C轮  ...   全职   \n",
      "474      3-5年   NaN      15-20K       互联网   NaN       0-20人    未融资  ...   全职   \n",
      "734  3天/周 3个月   NaN  100-150元/天   培训/辅导机构   NaN      20-99人    天使轮  ...   全职   \n",
      "473     5-10年   NaN      25-50K       互联网   NaN  1000-9999人  不需要融资  ...   全职   \n",
      "935      3-5年   NaN      15-30K       互联网   NaN    100-499人     A轮  ...   全职   \n",
      "\n",
      "                                                  岗位描述  \\\n",
      "47   熟悉C#，VB等全栈式开发，页面数据提取、文件数据处理、数据库调用、webservice/A...   \n",
      "776  岗位职责： 1.cv图像处理预识别，预标注，数据清洗入库2.数据拉流推流，爬虫采集2. 各类...   \n",
      "996  工作内容：负责与海外客户英文沟通确认需求负责RPA应用中定制化开发任务，确保RPA程序的功能...   \n",
      "279  1、 掌握C#编程；2、掌握WPF；3、掌握Windows API 、P/Invoke的运用...   \n",
      "168  岗位职责1、负责公司智能设备和物联网产品的上位机软件研发和维护工作；2、协助搭建公司的MES...   \n",
      "..                                                 ...   \n",
      "386  岗位职责：1. 带领公司BI团队2. 负责公司大数据分析平台搭建，相关数据的处理、分析、统计...   \n",
      "474  职责描述：1、负责配合项目经理实施智慧城市领域相关项目的技术实施和管理；2、编写、输出方案，...   \n",
      "734  1.负责完成公司交给的爬虫任务2.了解常见反爬3.实习为线上实习每天上午10点－12点，下午...   \n",
      "473  岗位职责：1.了解目前正在发展的大数据分布式平台前沿技术，开展大数据相关设计或研发工作；2....   \n",
      "935  岗位职责：1. 负责软件的研发、测试，包括业务需求的沟通，业务功能具体实现、单元测试、系统维...   \n",
      "\n",
      "                                                  公司介绍  \\\n",
      "47            主要从事业务流程自动化开发(RPA)，业务性能分析，IT服务管理软件开发和咨询。   \n",
      "776  中电信人工智能科技有限公司是中国电信开展人工智能业务的科技型、能力型、平台型专业公司，以央企...   \n",
      "996  广州龙驰电子科技有限公司成立于1999年，聚焦于银行、保险、交通的桌面解决方案。2004年成...   \n",
      "279                                         甲级设计院的设计团队   \n",
      "168  深圳百跑科技有限公司成立于2015年1月14日，为国家高新技术企业，是深圳市首批通过发改委认...   \n",
      "..                                                 ...   \n",
      "386  中商惠民成立于2013年，是国家现代服务业示范单位，国内快消品B2B智慧供应链领导品牌，同时...   \n",
      "474                                       智慧化建设、数字化应用。   \n",
      "734               一家哲学内容提供商 团队以北京大学本硕博为主目前主要做哲学的知识付费方向   \n",
      "473  京东科技集团是京东集团旗下专注于以技术为产业服务的业务子集团，致力于为企业、金融机构、政府等...   \n",
      "935  引跑科技是全球领先的云计算技术与服务提供商，拥有大数据平台（EngineOne?）、虚拟化平...   \n",
      "\n",
      "                                                公司工商信息  \\\n",
      "47   https://www.zhipin.com/gongsi/61197b2fddee80be...   \n",
      "776  https://www.zhipin.com/gongsi/ba9c66ffa8866512...   \n",
      "996  https://www.zhipin.com/gongsi/d54a098898bf7371...   \n",
      "279  https://www.zhipin.com/gongsi/13f7b3e3464ee04d...   \n",
      "168  https://www.zhipin.com/gongsi/5c39752c022686db...   \n",
      "..                                                 ...   \n",
      "386  https://www.zhipin.com/gongsi/14804a4a1306e4e0...   \n",
      "474  https://www.zhipin.com/gongsi/0feca5bee95df1bd...   \n",
      "734  https://www.zhipin.com/gongsi/ed36fead343b199c...   \n",
      "473  https://www.zhipin.com/gongsi/f9a5b0b441e18763...   \n",
      "935  https://www.zhipin.com/gongsi/c8a1df38780a0149...   \n",
      "\n",
      "                                               简历详情页地址                 更新日期  \\\n",
      "47   https://www.zhipin.com/job_detail/8ced2f70f72f...  2024-05-14 23:54:54   \n",
      "776  https://www.zhipin.com/job_detail/b30623fb0124...  2024-05-15 04:18:37   \n",
      "996  https://www.zhipin.com/job_detail/9820102e4065...  2024-05-15 08:51:27   \n",
      "279  https://www.zhipin.com/job_detail/4dcd6b6e7b0b...  2024-05-15 01:28:18   \n",
      "168  https://www.zhipin.com/job_detail/36920bf2f3d0...  2024-05-15 00:46:02   \n",
      "..                                                 ...                  ...   \n",
      "386  https://www.zhipin.com/job_detail/0b704fb2c06e...  2024-05-15 01:54:39   \n",
      "474  https://www.zhipin.com/job_detail/6fee288e0361...  2024-05-15 02:01:02   \n",
      "734  https://www.zhipin.com/job_detail/006eca900736...  2024-05-15 04:34:58   \n",
      "473  https://www.zhipin.com/job_detail/e8bc8fcccc5c...  2024-05-15 02:23:05   \n",
      "935  https://www.zhipin.com/job_detail/ddd77fa77bc1...  2024-05-15 04:56:54   \n",
      "\n",
      "                                                  工作内容  \\\n",
      "47   \\n岗位职责：\\n1. 熟悉C#和VB等全栈式开发技术。\\n2. 能够进行页面数据提取、文件...   \n",
      "776  \\n岗位职责：\\n1. CV图像处理预识别、预标注和数据清洗入库。\\n2. 数据拉流推流，爬...   \n",
      "996  \\n岗位职责：\\n1. 负责与海外客户进行英文沟通，确认需求。\\n2. 负责RPA应用中的定...   \n",
      "279  \\n岗位职责：\\n1. 使用C#进行桌面程序开发；\\n2. 应用WPF进行用户界面设计；\\n...   \n",
      "168  \\n岗位职责：\\n1. 负责公司智能设备和物联网产品的上位机软件研发和维护工作；\\n2. 协...   \n",
      "..                                                 ...   \n",
      "386  \\n岗位职责：\\n1. 带领公司BI团队\\n2. 负责公司大数据分析平台搭建，相关数据的处理...   \n",
      "474  \\n岗位职责：\\n1. 配合项目经理实施智慧城市领域相关项目的技术实施和管理；\\n2. 编写...   \n",
      "734  \\n岗位职责：\\n1. 完成公司分配的爬虫任务\\n2. 理解并应对常见的反爬策略\\n3. 进...   \n",
      "473  \\n岗位职责：\\n1. 了解目前正在发展的大数据分布式平台前沿技术，开展大数据相关设计或研发...   \n",
      "935  \\n岗位职责：\\n1. 负责软件的研发、测试，包括业务需求的沟通，业务功能具体实现、单元测试...   \n",
      "\n",
      "                                                  任职资格  \\\n",
      "47   \\n任职资格：\\n1. 熟悉C#，VB等全栈式开发。\\n2. 熟悉页面数据提取、文件数据处理...   \n",
      "776  \\n任职资格：\\n1. 本科以上学历，3年以上编程经验；\\n2. 精通Python语言，Sh...   \n",
      "996  \\n任职资格：\\n1. 英语六级以上，或具备英语口语交流能力。\\n2. 计算机相关专业，本科...   \n",
      "279  \\n任职资格：\\n1. 掌握C#编程；\\n2. 掌握WPF；\\n3. 掌握Windows A...   \n",
      "168  \\n任职资格：\\n1. 本科及以上学历，计算机或相关专业；\\n2. 熟悉使用C#或 .NET...   \n",
      "..                                                 ...   \n",
      "386  \\n任职资格：\\n1. 计算机科学、信息系统、数学或相关专业本科及以上学历。\\n2. 5年以...   \n",
      "474  \\n任职资格：\\n1. 全日制统招大学本科及以上学历，计算机等相关专业毕业；\\n2. 有5年...   \n",
      "734  \\n任职资格：\\n1. 熟悉爬虫技术，能够完成公司交给的爬虫任务。\\n2. 了解常见的反爬策...   \n",
      "473  \\n任职资格：\\n1. 本科及以上学历，计算机或数学相关专业；\\n2. 3年以上大数据相关项...   \n",
      "935  \\n任职资格：\\n1. 统招全日制本科计算机软件或相关专业毕业；\\n2. 3~10年开发经验...   \n",
      "\n",
      "                                                   技术栈 Cluster  \n",
      "47   \\n技术栈:\\n1. C#\\n2. VB\\n3. 页面数据提取\\n4. 文件数据处理\\n5....       0  \n",
      "776  \\n技术栈:\\n1. Python语言\\n2. Shell脚本\\n3. ffmpeg\\n4....       0  \n",
      "996  \\n技术栈:\\n1. 英语六级以上或英语口语交流能力\\n2. 计算机相关专业，本科及以上学历...       1  \n",
      "279  \\n技术栈:\\n1. C#\\n2. WPF\\n3. Windows API\\n4. P/In...       2  \n",
      "168  \\n技术栈:\\n1. C#\\n2. .NET\\n3. UART串口通信\\n4. TCP/UD...       2  \n",
      "..                                                 ...     ...  \n",
      "386  \\n技术栈:\\n1. Java\\n2. Scala\\n3. Python\\n4. SQL\\n...     914  \n",
      "474  \\n技术栈:\\n1. 计算机科学或相关领域的本科及以上学历\\n2. 全栈技术研发经验\\n3....     915  \n",
      "734                \\n技术栈:\\n1. 爬虫技术\\n2. 反爬策略\\n3. 线上工作模式     916  \n",
      "473  \\n技术栈:\\n1. Hadoop\\n2. Flink\\n3. Hive\\n4. Spark...     917  \n",
      "935  \\n技术栈:\\n1. Java\\n2. Python\\n3. 数据库技术\\n4. 缓存技术\\...     918  \n",
      "\n",
      "[1000 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# 使用层次聚类\n",
    "distance_threshold = 0.4 # 设置距离阈值（可以根据需要调整）\n",
    "hierarchical = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=distance_threshold)\n",
    "clusters_hier = hierarchical.fit_predict(cosine_distance_matrix)\n",
    "\n",
    "# 将聚类标签添加到原始 DataFrame 中\n",
    "data['Cluster'] = clusters_hier\n",
    "\n",
    "# 按照 Cluster 列升序排序\n",
    "data_sorted = data.sort_values(by='Cluster')\n",
    "\n",
    "# 输出结果\n",
    "print(data_sorted)\n",
    "\n",
    "# 如果需要保存结果为文件\n",
    "data_sorted.to_csv('cluster_word2vec.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert + cosin(职位名称)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def score(sentence1,sentence2):\n",
    "    # 初始化 BERT 模型和分词器\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-chinese')\n",
    "    model = BertModel.from_pretrained('bert-chinese')\n",
    "\n",
    "\n",
    "\n",
    "    # 分词并转换为模型输入\n",
    "    inputs1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "    inputs2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "    # 通过 BERT 模型获取嵌入\n",
    "    with torch.no_grad():\n",
    "        outputs1 = model(**inputs1)\n",
    "        outputs2 = model(**inputs2)\n",
    "\n",
    "    # 获取每个句子的嵌入（取所有隐藏层的平均值）\n",
    "    embeddings1 = torch.mean(outputs1.last_hidden_state, dim=1).numpy()\n",
    "    embeddings2 = torch.mean(outputs2.last_hidden_state, dim=1).numpy()\n",
    "\n",
    "    # 计算余弦相似度\n",
    "    similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "\n",
    "    print(f\"BERT Similarity: {similarity[0][0]}\")\n",
    "# 要比较的两个句子\n",
    "sentence1 = \"NET开发工程师（net Core）\"\n",
    "sentence2 = \".net开发工程师\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'Boss直聘/Boss直聘_skills.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "job_titles = data['职位名称'].tolist()\n",
    "\n",
    "\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-chinese')\n",
    "model = BertModel.from_pretrained('bert-chinese')\n",
    "\n",
    "def get_vector(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# 获取所有岗位名称的向量表示\n",
    "vectors = [get_vector(title) for title in job_titles]\n",
    "\n",
    "\n",
    "\n",
    "# 假设 job_titles 是你已加载的工作岗位名称列表\n",
    "# vectors_np 是对应的 BERT 向量表示\n",
    "vectors_np = np.array(vectors)\n",
    "cosine_distance_matrix = cosine_distances(vectors_np)\n",
    "\n",
    "# 设置相似度阈值，即距离阈值\n",
    "distance_threshold = 0.08 # 这个值可以根据你的需求调整\n",
    "\n",
    "# 使用层次聚类\n",
    "hierarchical = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=distance_threshold)\n",
    "clusters_hier = hierarchical.fit_predict(cosine_distance_matrix)\n",
    "\n",
    "\n",
    "# 将工作岗位名称和它们的聚类标签存入 DataFrame\n",
    "data['Cluster'] = clusters_hier\n",
    "\n",
    "# 按照 Cluster 列升序排序\n",
    "data_sorted = data.sort_values(by='Cluster')\n",
    "\n",
    "# 输出结果\n",
    "for index, row in data_sorted.iterrows():\n",
    "    print(f'Job Title: {row[\"职位名称\"]} - Cluster: {row[\"Cluster\"]}')\n",
    "\n",
    "# 如果需要保存结果为文件\n",
    "data_sorted.to_csv('clustered_job_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/wxs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/wxs/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m job_titles \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m职位名称\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 加载预训练的 BERT 模型和分词器\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-chinese\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-chinese\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keywords\u001b[39m(text, num_keywords\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# 使用spacy进行关键词提取\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1784\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m             resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m download_url(file_path, proxies\u001b[38;5;241m=\u001b[39mproxies)\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1784\u001b[0m         resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1800\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unresolved_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    414\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1219\u001b[0m     )\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1282\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m (url_to_download, etag, commit_hash, expected_size, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1721\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1642\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/file_download.py:395\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    396\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:66\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     68\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mconn\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/connectionpool.py:1099\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mproxy_is_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/connection.py:616\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n\u001b[1;32m    618\u001b[0m     tls_in_tls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/GLM3/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m     75\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 下载和加载nltk数据\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('chinese'))\n",
    "\n",
    "# 加载spacy模型\n",
    "nlp = spacy.load('zh_core_web_sm/zh_core_web_sm/zh_core_web_sm-3.7.0')\n",
    "\n",
    "# 加载CSV文件数据\n",
    "file_path = 'Boss直聘/Boss直聘_skills.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "job_titles = data['职位名称'].tolist()\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-chinese')\n",
    "model = BertModel.from_pretrained('bert-chinese')\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    # 使用spacy进行关键词提取\n",
    "    doc = nlp(text)\n",
    "    keywords = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.text not in stop_words]\n",
    "    word_freq = Counter(keywords)\n",
    "    common_words = word_freq.most_common(num_keywords)\n",
    "    return [word for word, freq in common_words]\n",
    "\n",
    "def get_vector(text):\n",
    "    keywords = extract_keywords(text)\n",
    "    concatenated_keywords = \" \".join(keywords)\n",
    "    inputs = tokenizer(concatenated_keywords, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# 获取所有岗位名称的向量表示\n",
    "vectors = [get_vector(title) for title in job_titles]\n",
    "\n",
    "# 转换为numpy数组\n",
    "vectors_np = np.array(vectors)\n",
    "cosine_distance_matrix = cosine_distances(vectors_np)\n",
    "\n",
    "# 设置相似度阈值，即距离阈值\n",
    "distance_threshold = 0.08  # 这个值可以根据你的需求调整\n",
    "\n",
    "# 使用层次聚类\n",
    "hierarchical = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=distance_threshold)\n",
    "clusters_hier = hierarchical.fit_predict(cosine_distance_matrix)\n",
    "\n",
    "# 将工作岗位名称和它们的聚类标签存入 DataFrame\n",
    "data['Cluster'] = clusters_hier\n",
    "\n",
    "# 按照 Cluster 列升序排序\n",
    "data_sorted = data.sort_values(by='Cluster')\n",
    "\n",
    "# 输出结果\n",
    "for index, row in data_sorted.iterrows():\n",
    "    print(f'Job Title: {row[\"职位名称\"]} - Cluster: {row[\"Cluster\"]}')\n",
    "\n",
    "# 如果需要保存结果为文件\n",
    "data_sorted.to_csv('clustered_Bert_spacy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert + cos(职位名称 + 岗位描述)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = 'Boss直聘/Boss直聘_skills.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 提取用于聚类的两列\n",
    "selected_columns = data[['职位名称', '技术栈']]  # 替换为实际的列名\n",
    "\n",
    "# 合并两列文本用于生成向量\n",
    "combined_text = (selected_columns['职位名称'] + ' ' + selected_columns['技术栈']).tolist()\n",
    "\n",
    "# 加载预训练的 BERT 模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-chinese')\n",
    "model = BertModel.from_pretrained('bert-chinese')\n",
    "\n",
    "# 对每个文本进行分词并生成 BERT 向量\n",
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "bert_vectors = [get_bert_embedding(text) for text in combined_text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算余弦距离\n",
    "cosine_distance_matrix = cosine_distances(bert_vectors)\n",
    "\n",
    "# 使用层次聚类，并替换 affinity 参数为 metric\n",
    "distance_threshold = 0.03  # 设置距离阈值（可以根据需要调整）\n",
    "hierarchical = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=distance_threshold)\n",
    "clusters_hier = hierarchical.fit_predict(cosine_distance_matrix)\n",
    "\n",
    "# 将聚类标签添加到原始 DataFrame 中\n",
    "data['Cluster'] = clusters_hier\n",
    "\n",
    "# 按照 Cluster 列升序排序\n",
    "data_sorted = data.sort_values(by='Cluster')\n",
    "\n",
    "# 输出结果\n",
    "print(data_sorted)\n",
    "\n",
    "# 如果需要保存结果为文件\n",
    "data_sorted.to_csv('clustered_job_titles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# 读取CSV文件，确保源文件是UTF-8编码\n",
    "file_path = 'change/Boss直聘--修_2.csv'\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# 使用“职位名称”列进行处理\n",
    "df['combined'] = df['工作内容'].fillna('')\n",
    "\n",
    "# 文本向量化\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('chinese')\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, max_features=1000)\n",
    "X = vectorizer.fit_transform(df['combined'])\n",
    "\n",
    "# 标准化数据\n",
    "scaler = StandardScaler(with_mean=False).fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# 网格搜索参数\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "for eps in np.arange(0.1, 1.1, 0.1):\n",
    "    for min_samples in range(2, 10):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "        labels = dbscan.fit_predict(X_scaled)\n",
    "        num_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        num_noise = list(labels).count(-1)\n",
    "        score = num_clusters - num_noise / float(len(labels))  # 这只是一个简单的指标，你可以用其他指标\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_eps = eps\n",
    "            best_min_samples = min_samples\n",
    "\n",
    "print(f'最佳eps: {best_eps}, 最佳min_samples: {best_min_samples}, 得分: {best_score}')\n",
    "\n",
    "# 用最佳参数再次运行DBSCAN\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples, metric='cosine')\n",
    "df['cluster'] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# 按照 cluster 列进行升序排序\n",
    "df_sorted = df.sort_values(by='cluster')\n",
    "\n",
    "# 将结果保存到CSV文件，确保输出文件编码为 UTF-8\n",
    "output_file_path = 'change/dbscan_clustered_sorted_jobs.csv'\n",
    "df_sorted.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f'已完成聚类，结果按cluster升序排序，并保存到 {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
