{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、api调用模型处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def call_large_model_api(user_prompt, user_input):\n",
    "    url = 'http://117.161.233.106:8000/v1/chat/completions?model=llama'  # 请替换为实际的API URL\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        # 请替换为实际的API密钥\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.01,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"repetition_penalty\": 10,\n",
    "        \"top_p\": 0.8,\n",
    "        \"do_sample\": True,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": user_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1000/1000 [13:18<00:00,  1.25row/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "# 定义一个函数来处理每一行数据\n",
    "def process_row(row):\n",
    "    # 这里是处理行的逻辑\n",
    "    # 例如，您可以提取入职要求和工作内容，并返回更新后的行\n",
    "    \n",
    "    try:\n",
    "        # 调用大模型API的逻辑（这里需要您自己实现call_large_model_api函数）\n",
    "        # prompt_1 = \"You are a Chinese wise man, always reply in simplified Chinese, not English, otherwise I will be very angry. Extract from the job description I gave you the job requirements or job requirements or what appears to be requirements. Rule: \\n- Extract from the job description, do not regenerate. \\n- Return format: 工作要求: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        # response = call_large_model_api(prompt_1, row[12])\n",
    "        # skill = response.json()\n",
    "        # skills = skill[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        # prompt_2 = \"You are a Chinese wise man, always reply in simplified Chinese, not English, otherwise I will be very angry.Extract the job description from the job description I gave you, not the entry requirement. Rule: \\n- Extract from the job description, do not regenerate. \\n- Return format: 工作要求: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        # content = call_large_model_api(prompt_2, row[12])\n",
    "        # content = content.json()\n",
    "        # content = content[\"choices\"][0][\"message\"][\"content\"]\n",
    "        prompt_3 = \"从任职要求提取技术栈\\n- Return format: 技术栈: \\n1. \\n2.... \\n - Always reply Simplified Chinese, not English, otherwise I will be very angry.\"\n",
    "        response = call_large_model_api(prompt_3, row[18])\n",
    "        skill = response.json()\n",
    "        skills = skill[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        skills = \"\"\n",
    "        # content = \"\"\n",
    "    \n",
    "    row.append(skills)\n",
    "    # row.append(content)\n",
    "    return row\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    with open('Boss直聘--修_1.csv', 'r', newline='') as csvfile, \\\n",
    "         open('Boss直聘--修_2.csv', 'w', newline='') as output_file:\n",
    "\n",
    "        reader = csv.reader(csvfile)\n",
    "        writer = csv.writer(output_file)\n",
    "        \n",
    "        # 写入标题行\n",
    "        writer.writerow(['职位名称', '工作地址','学历要求', '工作年限要求','招聘人数','薪资待遇','公司行业','公司性质','公司规模','融资阶段','招聘状态','职位类型','岗位描述','公司介绍','公司工商信息','简历详情页地址','更新日期','工作内容','入职要求','技术栈'])\n",
    "        next(reader)  # 跳过标题行\n",
    "        \n",
    "        # 初始化一个列表来存储所有的行，以便并行处理\n",
    "        rows = []\n",
    "        for row in reader:\n",
    "            rows.append(row)\n",
    "        \n",
    "        # 使用concurrent.futures并行处理数据\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            # 提交所有任务并获取future对象列表\n",
    "            futures = [executor.submit(process_row, row) for row in rows]\n",
    "            \n",
    "            # 初始化计数器和批处理列表\n",
    "            count = 0\n",
    "            batch = []\n",
    "            \n",
    "            # 遍历future对象，获取结果并写入文件\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing rows\", unit=\"row\"):\n",
    "                # 结果处理\n",
    "                processed_row = future.result()\n",
    "                batch.append(processed_row)\n",
    "                count += 1\n",
    "                \n",
    "                # 每100行，写入文件并清空批处理列表\n",
    "                if count % 100 == 0:\n",
    "                    writer.writerows(batch)\n",
    "                    batch.clear()\n",
    "\n",
    "# 运行主函数\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、整合相似工作岗位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "file_path = 'Boss直聘--修_2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to understand its structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sorted_out.csv'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cluster'] = df['职位名称'].str.lower().astype('category').cat.codes + 1\n",
    "\n",
    "# Sort the DataFrame by the 'cluster' column\n",
    "sorted_df = df.sort_values(by='cluster')\n",
    "\n",
    "# Save the sorted DataFrame to a new CSV file\n",
    "sorted_file_path = 'sorted_out.csv'\n",
    "sorted_df.to_csv(sorted_file_path, index=False)\n",
    "\n",
    "sorted_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to merge clusters based on job content similarity\n",
    "def merge_clusters(df, similarity_threshold=0.8):\n",
    "    # Calculate cosine similarity matrix\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['工作内容'])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    # Initialize an empty dictionary to store merged clusters\n",
    "    merged_clusters = {}\n",
    "\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for i in range(len(df)):\n",
    "        # If the row's cluster is not in the merged_clusters dictionary, add it\n",
    "        if df.at[i, 'cluster'] not in merged_clusters:\n",
    "            merged_clusters[df.at[i, 'cluster']] = df.at[i, 'cluster']\n",
    "        \n",
    "        # Compare the current row's job content with all other rows\n",
    "        for j in range(i+1, len(df)):\n",
    "            # If the cosine similarity is above the threshold and the clusters are different\n",
    "            if cosine_sim[i][j] > similarity_threshold and df.at[i, 'cluster'] != df.at[j, 'cluster']:\n",
    "                # Merge the clusters of the two rows\n",
    "                merged_cluster = min(merged_clusters[df.at[i, 'cluster']], merged_clusters[df.at[j, 'cluster']])\n",
    "                merged_clusters[df.at[i, 'cluster']] = merged_cluster\n",
    "                merged_clusters[df.at[j, 'cluster']] = merged_cluster\n",
    "\n",
    "    # Replace the original clusters with the merged clusters\n",
    "    df['cluster'] = df['cluster'].map(merged_clusters)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Merge clusters based on job content similarity\n",
    "df = merge_clusters(df)\n",
    "\n",
    "# Display the DataFrame with the updated 'cluster' column\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
